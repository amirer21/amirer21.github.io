---
title: AI - 선형 회귀와 로지스틱 회귀 핵심 정리
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
tags:
- AI
- MachineLearning
toc: true
toc_sticky: true
toc_label: 목차
description: 인공지능 - 선형 회귀와 로지스틱 회귀 핵심 정리
article_tag1: AI
article_tag2: MachineLearning
article_tag3: 
article_section: 
meta_keywords: AI, MachineLearning
last_modified_at: '2025-08-08 21:00:00 +0800'
---



### **AI 기초 다지기: x 핵심 정리 🤖**

인공지능과 머신러닝의 가장 기본이 되는 두 가지 모델, **선형 회귀**와 **로지스틱 회귀**에 대해 알아보겠습니다. 이 두 모델은 데이터의 패턴을 파악하고 미래를 예측하는 가장 직관적인 방법 중 하나입니다.

***

### **1. 선형 회귀: 데이터의 패턴을 직선으로 그리다 💡**

**선형 회귀(Linear Regression)**는 연속적인 값을 예측하는 데 사용됩니다. 예를 들어 '방의 크기'라는 데이터로 '집값'을 예측하는 것처럼, 하나의 변수가 다른 변수에 어떤 영향을 미치는지 파악하는 것이죠.

- **핵심 아이디어**: 데이터 포인트들을 가장 잘 나타내는 하나의 **직선**을 찾는 과정입니다.
- **수학적 표현**: 이 직선은 $Y = WX + b$ 라는 간단한 식으로 표현됩니다.
  - $X$: 입력 데이터 (예: 방 크기)
  - $Y$: 예측할 값 (예: 집값)
  - $W$: **가중치(Weight)** 또는 **계수(Coefficient)**. 직선의 **기울기**에 해당하며, $X$가 $Y$에 얼마나 큰 영향을 미치는지를 나타냅니다.
  - $b$: **편향(bias)** 또는 **절편(intercept)**. 직선이 y축과 만나는 지점으로, 기본적인 값을 보정해주는 역할을 합니다.

만약 입력 데이터 $X$의 종류(특성, feature)가 여러 개가 되면 어떻게 될까요? 예를 들어 '방 크기', '건축 연도', '역세권 여부' 3가지 특성으로 집값을 예측한다면, 데이터는 3차원 공간에 표현됩니다. 이 경우 모델은 직선이 아닌 **평면**을 찾게 됩니다. 특성이 4개 이상이 되면 눈으로 그릴 수는 없지만, 수학적으로는 더 높은 차원의 공간(**초평면, Hyperplane**)을 찾아 예측을 수행하게 됩니다.



***

### **2. 최적의 선을 찾는 기준, 손실 함수 🎯**

그렇다면 '데이터를 가장 잘 나타내는 직선'은 어떻게 찾을 수 있을까요? 바로 **오차(Error)**가 가장 작은 직선을 찾는 것입니다. 오차란 모델이 예측한 값($\hat{y}$)과 실제 데이터의 값($y$) 사이의 차이를 의미합니다.

이 전체 오차의 크기를 계산하는 함수를 **손실 함수(Loss Function)**라고 부릅니다. 선형 회귀의 목표는 이 손실 함수의 값을 최소로 만드는 최적의 가중치($W$)와 편향($b$)을 찾는 것입니다.

- **평균 제곱 오차 (Mean Squared Error, MSE)**: 가장 대표적인 손실 함수입니다. 각 데이터의 오차를 제곱한 뒤 평균을 낸 값입니다. 오차를 제곱하기 때문에, 오차가 클수록 더 큰 벌점(패널티)을 부여하는 효과가 있습니다.
  $$MSE = \frac{1}{m}\sum_{i=1}^{m}( y_i - \hat{y_i} )^2$$

***

### **3. '예' 또는 '아니오' 예측하기, 로지스틱 회귀 ✅**

만약 예측하려는 값이 '집값'처럼 연속된 숫자가 아니라, '합격/불합격', '스팸 메일/정상 메일'처럼 두 가지 중 하나로 나뉘는 **분류(Classification)** 문제라면 어떨까요?

이때 선형 회귀를 그대로 사용하면 예측값이 0보다 작거나 1보다 커지는 문제가 발생합니다. 이 문제를 해결하기 위해 등장한 것이 **로지스틱 회귀(Logistic Regression)**입니다.

- **핵심 아이디어**: 선형 회귀의 결과값을 **시그모이드(Sigmoid) 함수**에 통과시켜 0과 1 사이의 값으로 변환합니다.
- **시그모이드 함수**: 어떤 숫자가 들어와도 그 결과를 0과 1 사이의 S자 곡선 형태로 바꿔주는 함수입니다.
- **결과 해석**: 로지스틱 회귀의 출력값은 **'특정 클래스에 속할 확률'**로 해석됩니다. 예를 들어, 스팸 메일 분류 모델의 출력이 0.95라면 '이 메일이 스팸일 확률이 95%'라고 해석하는 것이죠.



> **중요!** 이름에 '회귀'가 들어가지만, 로지스틱 회귀는 실제로는 **분류 모델**입니다.

***

### **4. 파이썬으로 직접 만드는 선형 회귀 모델 📊**

Scikit-learn 라이브러리를 사용하면 선형 회귀 모델을 간단히 구현할 수 있습니다. (예: 당뇨병 진행도 예측 데이터)

1.  **데이터 준비**: 예측에 사용할 입력 데이터($X$)와 정답 데이터($y$)를 학습용과 테스트용으로 분리합니다.

2.  **모델 학습**: `LinearRegression` 모델을 불러와 `fit()` 메소드로 학습시킵니다. `model.fit(X_train, y_train)` 코드는 "X_train 데이터가 주어졌을 때 y_train 값이 나오도록 모델을 학습시켜라"는 의미입니다.

3.  **모델 해석 (가장 중요한 특성 찾기)**: 학습이 끝나면 모델의 **계수(`coef_`)**와 **절편(`intercept_`)** 값을 확인할 수 있습니다.
    - **`model.coef_`**: 각 특성(X)에 대한 가중치(W) 값들입니다. 이 **계수의 절댓값이 클수록** 해당 특성이 예측 결과에 더 큰 영향을 미친다는 의미입니다. 즉, 어떤 특성이 중요한지 파악하는 핵심 지표가 됩니다.

4.  **모델 평가**: `score()` 메소드로 모델의 성능을 평가합니다. 회귀 모델에서는 **결정계수($R^2$)** 값을 반환합니다.
    - **결정계수 ($R^2$)**: 0과 1 사이의 값을 가지며, 1에 가까울수록 모델이 데이터를 잘 설명하고 있다는 의미입니다.

5.  **시각화로 이해하기**: 여러 특성 중 **하나의 특성(예: BMI)**만 사용해 모델을 학습시키면, 2차원 그래프에 데이터와 학습된 회귀 직선을 함께 그려볼 수 있습니다. 이를 통해 선형 회귀가 어떻게 동작하는지 직관적으로 이해할 수 있습니다.