---
title: 예측에서 분류까지 - 선형 회귀를 S-커브로 바꾸는 시그모이드
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
- MachineLearning
tags:
- AI
- MachineLearning
toc: true
toc_sticky: true
toc_label: 목차
description: 선형 회귀를 S-커브로 바꾸는 시그모이드
article_tag1: AI
article_tag2: MachineLearning
article_tag3: 
article_section: 
meta_keywords: AI, MachineLearning
last_modified_at: '2025-08-08 21:00:00 +0800'
---


# **예측에서 분류까지: 선형 회귀를 S-커브로 바꾸는 시그모이드**

From Prediction to Classification: Sigmoid Transforms Linear Regression into S-Curves

통계학과 머신러닝의 세계에서 우리는 숫자를 예측하기도 하고, '예/아니오'와 같은 범주를 분류하기도 합니다. 이 두 가지 과제를 연결하는 핵심적인 다리 역할을 하는 개념들이 바로 **선형 회귀**, **시그모이드 함수**, 그리고 **S-커브**입니다.

***

### **1. 선형 회귀 (Linear Regression): 예측의 기본 직선**

**선형 회귀**는 변수들 사이의 **직선 관계를 찾아내는 통계 기법**입니다. 즉, 흩어져 있는 데이터 점들을 가장 잘 설명하는 하나의 직선을 긋는 과정이죠. 🏙️

예를 들어, '공부한 시간'에 따라 '시험 점수'가 어떻게 변할지 예측하는 모델을 만든다고 상상해 보세요. 선형 회귀는 이 데이터에 가장 적합한 직선 방정식을 찾아내고, 이를 통해 특정 공부 시간에 대한 예상 점수를 알려줍니다.

* **목표**: 연속적인 숫자(시험 점수, 집값 등)를 예측합니다.
* **형태**: 데이터를 가장 잘 나타내는 **직선**을 찾습니다.
* **방정식**: $Y = \beta_0 + \beta_1X + \epsilon$
    * $Y$: 예측하려는 값 (종속 변수, 예: 시험 점수)
    * $X$: 예측에 사용할 정보 (독립 변수, 예: 공부 시간)
    * $\beta_1$: 기울기. $X$가 1 증가할 때 $Y$가 얼마나 변하는지를 나타냅니다.
    * $\beta_0$: 절편. $X$가 0일 때의 기본 시작점입니다.

하지만 선형 회귀는 '합격/불합격'이나 '스팸/정상 메일'처럼 둘 중 하나를 결정하는 **분류 문제**에는 한계가 있습니다. 예측값이 0과 1의 범위를 훌쩍 벗어날 수 있기 때문이죠.

***

### **2. 시그모이드 함수 (Sigmoid Function): 확률의 언어로 번역하기**

이때 등장하는 것이 바로 **시그모이드 함수**입니다. 시그모이드 함수는 어떤 입력값이든 **0과 1 사이의 값으로 바꾸어주는** 마법 같은 함수입니다. 그 결과가 마치 S자 모양 같아 'S-커브'라고도 불립니다. 🎢



수학적으로는 다음과 같이 표현됩니다:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

이 함수의 핵심 특징은 **출력값이 항상 0과 1 사이**라는 점입니다. 이 덕분에 우리는 출력값을 **'확률'**로 해석할 수 있습니다. 예를 들어, 출력이 0.8이라면 '해당 사건이 일어날 확률이 80%'라고 이해하는 식이죠.

***

### **3. 둘의 만남: 로지스틱 회귀와 S-커브**

**선형 회귀**와 **시그모이드 함수**의 환상적인 조합은 **로지스틱 회귀(Logistic Regression)**에서 빛을 발합니다. 로지스틱 회귀는 분류 문제를 풀기 위해 다음과 같은 과정을 거칩니다.

1.  **일단 선형 회귀처럼 계산**: 예측에 사용할 정보($X$)들을 가지고 일단 직선의 방정식($z = \beta_0 + \beta_1X$)을 계산합니다. 이 결과값 $z$는 음수일 수도, 아주 큰 양수일 수도 있습니다.
2.  **시그모이드 함수 통과**: 위에서 얻은 결과 $z$를 시그모이드 함수에 넣어 **0과 1 사이의 확률값($p$)으로 변환**합니다.
    $$p = \sigma(z) = \frac{1}{1 + e^{-(\beta_0 + \beta_1X)}}$$

이제 우리는 최종적으로 확률($p$)을 얻었습니다! 이 확률이 0.5(50%)보다 크면 '합격', 작으면 '불합격'으로 분류할 수 있게 됩니다.

결론적으로, **시그모이드 함수**는 선형 회귀의 예측 결과(직선)를 확률을 나타내는 부드러운 **S-커브** 형태로 바꾸어, 머신러닝이 **분류 문제**를 풀 수 있도록 만들어주는 결정적인 다리(🌉) 역할을 합니다. 이를 통해 우리는 숫자를 예측하는 것을 넘어 '예/아니오'를 구분하는 더 넓은 범위의 문제를 해결할 수 있게 됩니다.
