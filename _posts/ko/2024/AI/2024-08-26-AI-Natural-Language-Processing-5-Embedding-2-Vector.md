---
title: 인공지능 - Vector - 임베딩에서 벡터란?
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
- NLP
tags:
- AI
- NLP
toc: true
toc_sticky: true
toc_label: 목차
description: 인공지능 - Vector - 임베딩에서 벡터란?
article_tag1: AI
article_tag2: NLP
article_tag3: Vector
article_section: 
meta_keywords: AI, NLP, Vector
last_modified_at: '2024-08-26 21:00:00 +0800'
---

### 임베딩에서 벡터란?

임베딩에서 **벡터**는 텍스트 데이터(예: 단어, 문장, 문서 등)를 수치화하여 고차원 공간에서 표현한 값입니다. 벡터는 일반적으로 다차원 배열(혹은 리스트)로, 각 차원은 텍스트의 특정한 의미적 특성을 나타냅니다. 예를 들어, 단어 벡터의 경우 특정 차원이 단어의 성별, 감정, 강도 등과 같은 의미적 특징을 표현할 수 있습니다.

이러한 벡터는 다음과 같은 중요한 역할을 합니다:

1. **의미적 유사성**: 고차원 공간에서 서로 가까운 거리에 위치한 벡터는 의미적으로 유사한 단어 또는 문장을 나타냅니다. 예를 들어, "고양이"와 "개"는 서로 다른 단어지만, 임베딩 벡터 공간에서는 가까운 위치에 놓일 수 있습니다.

2. **모델 학습의 입력**: 임베딩 벡터는 모델이 입력 데이터를 처리할 수 있도록 해줍니다. 원래 텍스트 데이터는 비정형 데이터이기 때문에 수치화가 필요하며, 임베딩 벡터를 통해 신경망 등의 모델이 학습을 진행할 수 있게 됩니다.

3. **차원 축소**: 임베딩은 일반적으로 원래 텍스트의 고차원 특성을 더 낮은 차원으로 압축하여 표현합니다. 이 과정에서 불필요한 정보는 제거되고, 중요한 의미만이 벡터에 남게 됩니다.

임베딩 벡터는 주로 신경망 기반의 모델(예: Word2Vec, GloVe, BERT 등)을 통해 생성되며, 각 벡터의 길이는 모델이 설정한 차원 수에 따라 달라집니다. 일반적으로는 100차원에서 1000차원 사이의 길이를 가지며, 이는 모델의 성능과 복잡도에 영향을 미칩니다.