---
title: ì¸ê³µì§€ëŠ¥ - Transformer (1)
author_profile: true
read_time: true
comments: true
share: true
related: true
categories:
- AI
- Transformer
tags:
- AI
- Transformer
toc: true
toc_sticky: true
toc_label: ëª©ì°¨
description: ì¸ê³µì§€ëŠ¥ - Transformer
article_tag1: AI
article_tag2: Transformer
article_tag3: 
article_section: 
meta_keywords: AI, LLM, Transformer
last_modified_at: '2024-08-25 21:00:00 +0800'
---

`Transformer`ëŠ” 2017ë…„ì— êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ "Attention is All You Need"ì—ì„œ ì†Œê°œëœ ì‹ ê²½ë§ ì•„í‚¤í…ì²˜ë¡œ, ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ì»´í“¨í„° ë¹„ì „(CV) ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ì„±ê³¼ë¥¼ ì´ë¤„ëƒˆìŠµë‹ˆë‹¤. `BERT`ì™€ `GPT`ì™€ ê°™ì€ ìµœì‹  ì–¸ì–´ ëª¨ë¸ë“¤ì€ ëª¨ë‘ Transformer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.

ì´ë²ˆ ë‹µë³€ì—ì„œëŠ” Transformerì˜ ê¸°ë³¸ ê°œë…ê³¼ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ê³ , ê°„ë‹¨í•œ ì˜ˆì œ ì½”ë“œë¥¼ í†µí•´ ì´í•´ë¥¼ ë„ìš¸ ê²ƒì…ë‹ˆë‹¤.

## ğŸ” Transformerë€ ë¬´ì—‡ì¸ê°€?

TransformerëŠ” ìˆœì°¨ì ì¸ ë°ì´í„° ì²˜ë¦¬ì— ì í•©í•œ ê¸°ì¡´ì˜ RNN(Recurrent Neural Network)ì´ë‚˜ CNN(Convolutional Neural Network)ê³¼ ë‹¬ë¦¬, **Self-Attention ë©”ì»¤ë‹ˆì¦˜**ì„ í™œìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ê³  ê¸´ ë¬¸ë§¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

- **ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥**: ìˆœì°¨ì ì¸ ì²˜ë¦¬ì— ì˜ì¡´í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ê¸´ ì˜ì¡´ì„± í•™ìŠµ**: ë¬¸ì¥ ë‚´ì˜ ë¨¼ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ë” ì •í™•í•œ ë¬¸ë§¥ ì´í•´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- **ìœ ì—°í•œ êµ¬ì¡°**: Encoderì™€ Decoderë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ ë‹¤ì–‘í•œ ì‘ì—…ì— ë§ê²Œ ì¡°ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ğŸ—ï¸ Transformerì˜ êµ¬ì„± ìš”ì†Œ

TransformerëŠ” í¬ê²Œ **Encoder**ì™€ **Decoder** ë‘ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

### 1. Encoder

- ì…ë ¥ ë¬¸ì¥ì„ ë°›ì•„ ë‚´ë¶€ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- ì—¬ëŸ¬ ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ë©°, ê° ë ˆì´ì–´ëŠ” **Self-Attention**ê³¼ **í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§(Feed-Forward Neural Network)**ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.

### 2. Decoder

- Encoderì˜ ì¶œë ¥ì„ ë°›ì•„ ëª©í‘œ ì¶œë ¥(sequence)ì„ ìƒì„±í•©ë‹ˆë‹¤.
- ì—­ì‹œ ì—¬ëŸ¬ ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ë©°, ê° ë ˆì´ì–´ëŠ” **Self-Attention**, **Encoder-Decoder Attention**, ê·¸ë¦¬ê³  **í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§**ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

### 3. Self-Attention ë©”ì»¤ë‹ˆì¦˜

- ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
- ê° ë‹¨ì–´ê°€ ë¬¸ì¥ ë‚´ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì–´ë–»ê²Œ ì°¸ì¡°í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ì—¬ ë” í’ë¶€í•œ í‘œí˜„ì„ ì œê³µí•©ë‹ˆë‹¤.

### 4. Multi-Head Attention

- ì—¬ëŸ¬ ê°œì˜ Self-Attentionì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ì–‘í•œ í‘œí˜„ ê³µê°„ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
- ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ë™ì‹œì— í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 5. Position Encoding

- ìˆœì„œ ì •ë³´ê°€ ì—†ëŠ” ì…ë ¥ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

## ğŸ§ª ì˜ˆì œ ì½”ë“œ

ì´ì œ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ Transformer ëª¨ë¸ì„ êµ¬í˜„í•˜ëŠ” ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install torch torchvision torchtext
```

### ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator
```

### ë°ì´í„° ì „ì²˜ë¦¬

```python
SRC = Field(tokenize="spacy", tokenizer_language="de", init_token='<sos>', eos_token='<eos>', lower=True)
TRG = Field(tokenize="spacy", tokenizer_language="en", init_token='<sos>', eos_token='<eos>', lower=True)

train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))

SRC.build_vocab(train_data, min_freq=2)
TRG.build_vocab(train_data, min_freq=2)
```

### ëª¨ë¸ ì •ì˜

```python
class TransformerModel(nn.Module):
    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=512, num_heads=8, num_layers=3, forward_expansion=4, dropout=0.1, max_len=100):
        super(TransformerModel, self).__init__()
        
        self.src_word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.src_position_embedding = nn.Embedding(max_len, embed_size)
        
        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.trg_position_embedding = nn.Embedding(max_len, embed_size)
        
        self.transformer = nn.Transformer(embed_size, num_heads, num_layers, num_layers, forward_expansion * embed_size, dropout)
        
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        
        self.dropout = nn.Dropout(dropout)
        
        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        
    def make_src_mask(self, src):
        src_mask = src.transpose(0, 1) == self.src_pad_idx
        return src_mask
    
    def make_trg_mask(self, trg):
        trg_len = trg.shape[0]
        trg_mask = self.transformer.generate_square_subsequent_mask(trg_len).to(trg.device)
        return trg_mask
    
    def forward(self, src, trg):
        src_seq_length, N = src.shape
        trg_seq_length, N = trg.shape
        
        src_positions = torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(src.device)
        trg_positions = torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(trg.device)
        
        embed_src = self.dropout(self.src_word_embedding(src) + self.src_position_embedding(src_positions))
        embed_trg = self.dropout(self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))
        
        src_padding_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        
        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask)
        out = self.fc_out(out)
        
        return out
```

### ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ ì„¤ì •

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

src_vocab_size = len(SRC.vocab)
trg_vocab_size = len(TRG.vocab)
src_pad_idx = SRC.vocab.stoi['<pad>']
trg_pad_idx = TRG.vocab.stoi['<pad>']

model = TransformerModel(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)

optimizer = optim.Adam(model.parameters(), lr=0.0005)
criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)
```

### í•™ìŠµ ë£¨í”„

```python
for epoch in range(10):
    model.train()
    
    for batch in train_iterator:
        src = batch.src.to(device)
        trg = batch.trg.to(device)
        
        output = model(src, trg[:-1, :])
        output = output.reshape(-1, output.shape[2])
        trg = trg[1:, :].reshape(-1)
        
        optimizer.zero_grad()
        loss = criterion(output, trg)
        loss.backward()
        optimizer.step()
        
    print(f'Epoch {epoch} Loss {loss.item():.4f}')
```

### ì˜ˆì œ ì„¤ëª…

1. **ë°ì´í„° ì „ì²˜ë¦¬**: `torchtext`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…ì¼ì–´-ì˜ì–´ ë²ˆì—­ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
2. **ëª¨ë¸ ì •ì˜**: `TransformerModel` í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ì—¬ Transformer ì•„í‚¤í…ì²˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
3. **ëª¨ë¸ ì´ˆê¸°í™”**: ì–´íœ˜ í¬ê¸°, íŒ¨ë”© ì¸ë±ìŠ¤ ë“±ì„ ì„¤ì •í•˜ê³  ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
4. **í•™ìŠµ ë£¨í”„**: ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ë©° ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

ì´ ì˜ˆì œëŠ” Transformerì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ êµ¬í˜„ì´ë©°, ì‹¤ì œë¡œëŠ” ë” ë§ì€ íŠœë‹ê³¼ ë°ì´í„° ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

## ğŸ”— BERTì™€ GPTì—ì„œì˜ Transformer

### BERT (Bidirectional Encoder Representations from Transformers)

- **êµ¬ì¡°**: Transformerì˜ **Encoder** ë¶€ë¶„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **íŠ¹ì§•**:
  - ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ì„ ì´í•´í•©ë‹ˆë‹¤.
  - ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§(Masked Language Modeling)ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
- **ìš©ë„**:
  - ë¬¸ì¥ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ì§ˆë¬¸ ì‘ë‹µ ë“± ë‹¤ì–‘í•œ NLP ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤.

### GPT (Generative Pretrained Transformer)

- **êµ¬ì¡°**: Transformerì˜ **Decoder** ë¶€ë¶„ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **íŠ¹ì§•**:
  - ë‹¨ë°©í–¥(ìˆœë°©í–¥)ìœ¼ë¡œ ë¬¸ë§¥ì„ ì´í•´í•©ë‹ˆë‹¤.
  - ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì–¸ì–´ ëª¨ë¸ë§ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.
- **ìš©ë„**:
  - í…ìŠ¤íŠ¸ ìƒì„±, ë²ˆì—­, ìš”ì•½ ë“± ìƒì„± ê´€ë ¨ ì‘ì—…ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

## ğŸ§ ê²°ë¡ 

TransformerëŠ” í˜„ëŒ€ NLPì˜ í•µì‹¬ì ì¸ ì•„í‚¤í…ì²˜ë¡œ, ê·¸ ìœ ì—°ì„±ê³¼ ì„±ëŠ¥ìœ¼ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ ì‘ì—…ì—ì„œ í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ë‹µë³€ì—ì„œëŠ” Transformerì˜ ê¸°ë³¸ ê°œë…ê³¼ êµ¬ì¡°ë¥¼ ì‚´í´ë³´ê³ , ê°„ë‹¨í•œ êµ¬í˜„ ì˜ˆì œë¥¼ í†µí•´ ì´í•´ë¥¼ ë„ì™”ìŠµë‹ˆë‹¤. ë” ê¹Šì´ ìˆëŠ” ì´í•´ë¥¼ ìœ„í•´ì„œëŠ” ê³µì‹ ë…¼ë¬¸ê³¼ ë‹¤ì–‘í•œ êµ¬í˜„ ì˜ˆì œë¥¼ ì°¸ê³ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

## ğŸ“š ì°¸ê³  ìë£Œ

- [Attention is All You Need ë…¼ë¬¸](https://arxiv.org/abs/1706.03762)
- [PyTorch Transformer íŠœí† ë¦¬ì–¼](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [BERT ë…¼ë¬¸](https://arxiv.org/abs/1810.04805)
- [GPT ë…¼ë¬¸](https://openai.com/research/language-unsupervised)